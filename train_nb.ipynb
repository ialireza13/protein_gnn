{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_pytorch_version(version):\n",
    "#   return version.split('+')[0]\n",
    "\n",
    "# TORCH_version = torch.__version__\n",
    "# TORCH = format_pytorch_version(TORCH_version)\n",
    "\n",
    "# def format_cuda_version(version):\n",
    "#   return 'cu' + version.replace('.', '')\n",
    "\n",
    "# CUDA_version = torch.version.cuda\n",
    "# CUDA = format_cuda_version(CUDA_version)\n",
    "\n",
    "# !pip install tqdm\n",
    "# !pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "# !pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "# !pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "# !pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "# !pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Dataset, download_url, DataLoader\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = ''\n",
    "db_path = 'pt_cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, GINConv, GATConv, SAGEConv, GATv2Conv\n",
    "import glob\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class CustomGATLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_heads):\n",
    "        super(CustomGATLayer, self).__init__()\n",
    "        self.multihead_attention = GATConv(\n",
    "            in_dim, out_dim, heads=num_heads, concat=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(out_dim * num_heads)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.multihead_attention(x, edge_index)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomGATModel(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, num_classes, num_layers, num_heads, dict_size, num_fc_layers):\n",
    "        super(CustomGATModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(dict_size, in_dim)\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            gat_layer = CustomGATLayer(in_dim, hidden_dim, num_heads)\n",
    "            self.gat_layers.append(gat_layer)\n",
    "            in_dim = hidden_dim * num_heads  # Update input dimension for the next layer\n",
    "        \n",
    "        self.fc = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_fc_layers-1):\n",
    "            self.fc.append(nn.Linear(hidden_dim * num_heads, hidden_dim * num_heads))\n",
    "\n",
    "        self.fc.append(nn.Linear(hidden_dim * num_heads, hidden_dim))\n",
    "        self.fc.append(nn.Linear(hidden_dim, num_classes))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x.int(), data.edge_index.int()\n",
    "        x = self.embedding(x)[:, 0, :]\n",
    "        for gat_layer in self.gat_layers:\n",
    "            x = F.relu(gat_layer(x, edge_index))\n",
    "        \n",
    "        for fc_layer in self.fc[:-1]:\n",
    "            x = F.relu(fc_layer(x))\n",
    "        \n",
    "        logits = self.fc[-1](x)\n",
    "            \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_graph_size = 2000\n",
    "\n",
    "# # List of file paths to the data\n",
    "# file_paths = os.listdir(db_path)\n",
    "\n",
    "# df = pd.read_csv('size_data.csv', index_col=0)\n",
    "# df.file = df.file.apply(lambda x: x.split('/')[-1].split('.')[0] + '.pt')\n",
    "\n",
    "# subset = df[(df['size'] <= max_graph_size) & (df.file.isin(file_paths))]\n",
    "\n",
    "# file_paths = subset.file.values.tolist()\n",
    "\n",
    "# dataset = []\n",
    "# for x in tqdm(file_paths):\n",
    "#     dataset.append([x.split('.')[0], torch.load(db_path+x)])\n",
    "    \n",
    "# pickle.dump(dataset, file = open(\"dataset.pickle\", \"wb\"))\n",
    "\n",
    "dataset = pickle.load(open(\"dataset.pickle\", \"rb\"))\n",
    "protein_names = [x[0] for x in dataset]\n",
    "dataset = [x[1] for x in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(\n",
    "    n_heads = 8,\n",
    "    n_layers = 4,\n",
    "    embed_dim = 256,\n",
    "    n_fc_layers = 1,\n",
    "    batch_size = 32,\n",
    "    early_stopping_epochs = 10,\n",
    "    lr = 0.001,\n",
    "    clip = 1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "train_ratio = 0.8\n",
    "split_idx = int(train_ratio * len(dataset))\n",
    "\n",
    "train_files = dataset[:split_idx]\n",
    "test_files = dataset[split_idx:]\n",
    "\n",
    "model = CustomGATModel(32, hyperparameters['embed_dim'], 3, hyperparameters['n_layers'],\n",
    "                       hyperparameters['n_heads'], max_graph_size+1, hyperparameters['n_fc_layers']).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=hyperparameters['lr'])\n",
    "\n",
    "T_max = 32\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.278755"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "get_n_params(model)/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_files, batch_size=hyperparameters['batch_size'], shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_files, batch_size=hyperparameters['batch_size'], shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "prev_loss_test = 1e100\n",
    "es_epochs = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_test = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            outputs = model(data.to(device))\n",
    "            loss_test += criterion(outputs, data.y)\n",
    "\n",
    "            total += data.y.size(0)\n",
    "            correct += (outputs.argmax(dim=1) == data.y).sum().item()\n",
    "\n",
    "    accuracy_test = correct / total\n",
    "    best_yet = ' '\n",
    "\n",
    "    if loss_test < prev_loss_test:\n",
    "        es_spochs = 0\n",
    "        prev_loss_test = loss_test\n",
    "        torch.save(model, root + 'best_model.pt')\n",
    "        best_yet = '*'\n",
    "    else:\n",
    "        es_epochs += 1\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_train = 0\n",
    "    with torch.no_grad():\n",
    "        for data in train_loader:\n",
    "            outputs = model(data.to(device))\n",
    "            loss_train += criterion(outputs, data.y)\n",
    "\n",
    "            total += data.y.size(0)\n",
    "            correct += (outputs.argmax(dim=1) == data.y).sum().item()\n",
    "\n",
    "    accuracy_train = correct / total\n",
    "\n",
    "    print(f'Epoch: {epoch}/{epochs}\\tTrain Accuracy: {accuracy_train:.2f}\\tTest Accuracy: {accuracy_test:.2f}\\t' + best_yet + f'\\tLoss Train: {loss_train:.2f}\\tLoss Test: {loss_test:.2f}', file=open(\"output.txt\", \"a\"))\n",
    "    print(f'Epoch: {epoch}/{epochs}\\tTrain Accuracy: {accuracy_train:.2f}\\tTest Accuracy: {accuracy_test:.2f}\\t' + best_yet + f'\\tLoss Train: {loss_train:.2f}\\tLoss Test: {loss_test:.2f}')\n",
    "\n",
    "    for data in train_loader:\n",
    "        outputs = model(data.to(device))\n",
    "\n",
    "        loss = criterion(outputs, data.y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), hyperparameters['clip'])\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if es_epochs > hyperparameters['early_stopping_epochs']:\n",
    "        print('Early Stopping...')\n",
    "        print('Early Stopping...', file=open(\"output.txt\", \"a\"))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
